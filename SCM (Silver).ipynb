{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30899c53-4ecd-49e8-b7d6-b6e72dc611cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from pyspark.sql.functions import to_date, date_format, coalesce, col,count, when, create_map, lit\n",
    "from pyspark.sql.types import DateType\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bd28f95-0817-4dfc-b508-79fc3d2827fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "creating spark section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fff343f2-961a-49c9-8513-4bfcfddde53f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DeltaTableCheckAndAppend\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5481cbdd-e966-409a-a8f5-0d6cd69a1490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce532715-0174-4482-b836-d5ada09dcf00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define paths\n",
    "bronze_path = \"abfss://bronze@scmdataset2025.dfs.core.windows.net/Static_Data/Customer_Data.csv\"\n",
    "silver_path = \"abfss://silver@scmdataset2025.dfs.core.windows.net/Customer\"\n",
    "\n",
    "# Function to check if Delta table exists\n",
    "def delta_table_exists(path):\n",
    "    try:\n",
    "        files = dbutils.fs.ls(path)\n",
    "        return any(file.name == \"_delta_log/\" for file in files)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to access path: {e}\")\n",
    "        return False\n",
    "\n",
    "# If Delta table already exists, exit early\n",
    "if delta_table_exists(silver_path):\n",
    "    print(\"Delta table already exists in the silver layer.\")\n",
    "else:\n",
    "    # Read the CSV file\n",
    "    df_customers = spark.read.format(\"csv\").options(header=\"true\", inferSchema=\"true\").load(bronze_path)\n",
    "\n",
    "    # Remove duplicates based on Customer_ID\n",
    "    df_customers = df_customers.dropDuplicates([\"Customer_ID\"])\n",
    "\n",
    "    # Drop the \"Customer_Name\" column\n",
    "    df_customers = df_customers.drop(\"Customer_Name\")\n",
    "\n",
    "    # Write the DataFrame to Delta table in the silver layer\n",
    "    df_customers.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(silver_path)\n",
    "\n",
    "    # Optimize Delta table to maintain a single file\n",
    "    spark.sql(f\"OPTIMIZE delta.`{silver_path}`\")\n",
    "\n",
    "    print(\"File saved in the silver layer\")\n",
    "    print(\"Data processed successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54f68cc0-69c8-41d3-baa5-945ac8fd495d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05be1fbe-fa52-4ee1-9efa-eb9787b81dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the Delta table path\n",
    "delta_path = \"abfss://silver@scmdataset2025.dfs.core.windows.net/Product\"\n",
    "\n",
    "# Check if _delta_log directory exists\n",
    "def delta_table_exists(path):\n",
    "    try:\n",
    "        files = dbutils.fs.ls(path)\n",
    "        return any(file.name == \"_delta_log/\" for file in files)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to access path: {e}\")\n",
    "        return False\n",
    "    \n",
    "# if delta_log not exist\n",
    "if not delta_table_exists(delta_path):\n",
    "    # Read the CSV file\n",
    "    df_product = spark.read.format(\"csv\")\\\n",
    "        .options(header=\"true\")\\\n",
    "        .options(inferSchema=\"true\")\\\n",
    "        .load(\"abfss://bronze@scmdataset2025.dfs.core.windows.net/Static_Data/Product_Data.csv\")\n",
    "\n",
    "    # Create a temporary view for the Product data\n",
    "    df_product.createOrReplaceTempView(\"temp_product_view\")\n",
    "\n",
    "    # Drop duplicates based on Product_ID\n",
    "    df_product = df_product.dropDuplicates([\"Product_ID\"])\n",
    "    display(df_product)\n",
    "\n",
    "    # Write data to Delta table \n",
    "    print(\"Writing data to Delta table...\")\n",
    "    df_product.write.format(\"delta\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .option(\"mergeSchema\", \"true\")\\\n",
    "        .save(delta_path)\n",
    "\n",
    "    # Optimize Delta table to maintain a single file\n",
    "    spark.sql(f\"OPTIMIZE delta.`{delta_path}`\")\n",
    "\n",
    "    print(\"Data processed successfully.\")\n",
    "else:\n",
    "    # If Delta table already exists, do nothing\n",
    "    print(\"Delta table already exists. Skipping data processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a27dcc-df76-4f85-8a49-6cc362145dfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0a04216-a36f-4d5b-93bd-d746b154a683",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"abfss://bronze@scmdataset2025.dfs.core.windows.net/Stream_Data/Warehouse/\"\n",
    "\n",
    "df_warehouse = (spark.readStream\n",
    "      .format(\"cloudFiles\")\n",
    "      .option(\"cloudFiles.format\", \"csv\")\n",
    "      .option(\"cloudFiles.schemaLocation\", \"abfss://checkpoint@scmdataset2025.dfs.core.windows.net/warehouse_schema/\")\n",
    "      .option(\"cloudFiles.inferColumnTypes\", \"true\") \n",
    "      .load(base_path)  \n",
    "     )\n",
    "\n",
    "#creating tempview\n",
    "df_warehouse.createOrReplaceTempView(\"temp_warehouse_view\")\n",
    "\n",
    "# Adjust the date format in the 'Date' column\n",
    "df_warehouse = df_warehouse.withColumn(\n",
    "    \"Date\",\n",
    "    date_format(\n",
    "        coalesce(\n",
    "            to_date(col(\"Date\"), \"dd-MM-yyyy\"),\n",
    "            to_date(col(\"Date\"), \"dd MM yyyy\"),\n",
    "            to_date(col(\"Date\"), \"yyyy-MM-dd\"),\n",
    "            to_date(col(\"Date\"), \"yyyy-dd-MM\"),\n",
    "            to_date(col(\"Date\"), \"yyyy_dd_MM\"),\n",
    "            to_date(col(\"Date\"), \"dd_MM_yyyy\")\n",
    "        ),\n",
    "        \"dd-MM-yyyy\"\n",
    "    )\n",
    ")\n",
    "df_warehouse = df_warehouse.withColumn(\"Date\", col(\"Date\").cast(DateType()))\n",
    "\n",
    "\n",
    "# Create a mapping dictionary for Warehouse ID and Location\n",
    "warehouse_mapping = {\n",
    "    \"W001\": \"New_Delhi\",\n",
    "    \"W002\": \"Lucknow\",\n",
    "    \"W003\": \"Chandigarh\",\n",
    "    \"W004\": \"Mumbai\",\n",
    "    \"W005\": \"Ahmedabad\",\n",
    "    \"W006\": \"Jaipur\",\n",
    "    \"W007\": \"Kolkata\",\n",
    "    \"W008\": \"Bhubaneswar\",\n",
    "    \"W009\": \"Patna\",\n",
    "    \"W010\": \"Bhopal\",\n",
    "    \"W011\": \"Raipur\",\n",
    "    \"W012\": \"Ranchi\",\n",
    "    \"W013\": \"Bangalore\",\n",
    "    \"W014\": \"Hyderabad\",\n",
    "    \"W015\": \"Chennai\"\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a PySpark map\n",
    "mapping_expr = create_map([lit(x) for x in chain(*warehouse_mapping.items())])\n",
    "\n",
    "# Replace values directly in the Warehouse_Location column\n",
    "df_warehouse = df_warehouse.withColumn(\n",
    "    \"Warehouse_Location\",\n",
    "    when(\n",
    "        col(\"Warehouse_ID\").isNotNull() & (df_warehouse[\"Warehouse_Location\"] != mapping_expr.getItem(col(\"Warehouse_ID\"))),\n",
    "        mapping_expr.getItem(col(\"Warehouse_ID\"))\n",
    "    ).otherwise(col(\"Warehouse_Location\"))\n",
    ")\n",
    "\n",
    "# save file in silver layer\n",
    "df_warehouse.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"abfss://checkpoint@scmdataset2025.dfs.core.windows.net/warehouse_load/\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start(\"abfss://silver@scmdataset2025.dfs.core.windows.net/Warehouse/\") \\\n",
    "    .awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bacbf31e-fe4d-4ff1-b8cc-df3702eee8d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Supply dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f70f9d4d-98b7-40dd-a050-701a7a9db350",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"abfss://bronze@scmdataset2025.dfs.core.windows.net/Stream_Data/Supply/\"\n",
    "\n",
    "df_supply = (spark.readStream\n",
    "      .format(\"cloudFiles\")\n",
    "      .option(\"cloudFiles.format\", \"csv\")\n",
    "      .option(\"cloudFiles.schemaLocation\", \"abfss://checkpoint@scmdataset2025.dfs.core.windows.net/Supply_schema/\")\n",
    "      .option(\"cloudFiles.inferColumnTypes\", \"true\")  \n",
    "      .load(base_path)  \n",
    "     )\n",
    "\n",
    "df_supply.createOrReplaceTempView(\"temp_view_supply\")\n",
    "\n",
    "#drop supplier name column\n",
    "df_1= df_supply.drop(\"Supplier_Name\")\n",
    "\n",
    "# Adjust the date format in the 'Date' column\n",
    "df_supply = df_1.withColumn(\n",
    "    \"Date\",\n",
    "    date_format(\n",
    "        coalesce(\n",
    "            to_date(col(\"Date\"), \"dd-MM-yyyy\"),\n",
    "            to_date(col(\"Date\"), \"dd MM yyyy\"),\n",
    "            to_date(col(\"Date\"), \"yyyy-MM-dd\"),\n",
    "            to_date(col(\"Date\"), \"yyyy-dd-MM\"),\n",
    "            to_date(col(\"Date\"), \"yyyy_dd_MM\"),\n",
    "            to_date(col(\"Date\"), \"dd_MM_yyyy\")\n",
    "        ),\n",
    "        \"dd-MM-yyyy\"\n",
    "    )\n",
    ")\n",
    "df_supply = df_supply.withColumn(\"Date\", col(\"Date\").cast(DateType()))\n",
    "\n",
    "\n",
    "\n",
    "# Define the correct mapping of Supplier_ID to Supplier_State\n",
    "correct_mapping = {\n",
    "    \"S001\": \"Delhi\",\n",
    "    \"S002\": \"Maharashtra\",\n",
    "    \"S003\": \"West_Bengal\",\n",
    "    \"S004\": \"Madhya_Pradesh\",\n",
    "    \"S005\": \"Karnataka\"\n",
    "}\n",
    "\n",
    "# Create a DataFrame with the correct mapping\n",
    "mapping_df = spark.createDataFrame(\n",
    "    [(k, v) for k, v in correct_mapping.items()],\n",
    "    [\"Supplier_ID\", \"Correct_Supplier_State\"]\n",
    ")\n",
    "\n",
    "# Join the original DataFrame with the mapping DataFrame\n",
    "df_check = df_supply.join(mapping_df, on=\"Supplier_ID\", how=\"left\")\n",
    "\n",
    "# Replace incorrect values in 'Supplier_State' directly\n",
    "df_check = df_check.withColumn(\n",
    "    \"Supplier_State\",\n",
    "    when(df_check[\"Supplier_State\"] != df_check[\"Correct_Supplier_State\"], df_check[\"Correct_Supplier_State\"])\n",
    "    .otherwise(df_check[\"Supplier_State\"])\n",
    ").drop(\"Correct_Supplier_State\")  # Remove the extra column\n",
    "\n",
    "\n",
    "\n",
    "df_check.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"abfss://checkpoint@scmdataset2025.dfs.core.windows.net/Supply_load/\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start(\"abfss://silver@scmdataset2025.dfs.core.windows.net/Supply/\") \\\n",
    "    .awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a449ee8-66d1-4fa7-9fc1-9523d98700be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61c125a8-344b-471b-9e9c-a468ee35b9a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DateType, StructType, StructField, StringType, IntegerType, DoubleType\n",
    "import json\n",
    "\n",
    "base_path = \"abfss://bronze@scmdataset2025.dfs.core.windows.net/Stream_Data/Orders/\"\n",
    "df_orders = (spark.readStream\n",
    "      .format(\"cloudFiles\")\n",
    "      .option(\"cloudFiles.format\", \"csv\")\n",
    "      .option(\"cloudFiles.schemaLocation\", \"abfss://checkpoint@scmdataset2025.dfs.core.windows.net/Orders_schema/\")\n",
    "      .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "      .load(base_path)  \n",
    "     )\n",
    "\n",
    "df_orders.createOrReplaceTempView(\"temp_view_supply\")\n",
    "\n",
    "df_orders = df_orders.withColumn(\"Order_Date\", col(\"Order_Date\").cast(DateType()))\n",
    "\n",
    "df_orders = df_orders.dropDuplicates()\n",
    "\n",
    "df_orders.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"abfss://checkpoint@scmdataset2025.dfs.core.windows.net/Orders_load/\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start(\"abfss://silver@scmdataset2025.dfs.core.windows.net/Orders/\") \\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c9d3509-d5a3-444d-87de-371d7f2efff3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Logistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fa3af0a-dd7d-4109-9579-8cab32b6fa79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"abfss://bronze@scmdataset2025.dfs.core.windows.net/Stream_Data/Logistics/\"\n",
    "\n",
    "df_Logistics = (spark.readStream\n",
    "      .format(\"cloudFiles\")\n",
    "      .option(\"cloudFiles.format\", \"csv\")\n",
    "      .option(\"cloudFiles.schemaLocation\", \"abfss://checkpoint@scmdataset2025.dfs.core.windows.net/Logistics_schema/\")\n",
    "      .option(\"cloudFiles.inferColumnTypes\", \"true\") \n",
    "      .load(base_path)  \n",
    "     )\n",
    "\n",
    "\n",
    "df_Logistics.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"abfss://checkpoint@scmdataset2025.dfs.core.windows.net/Logistics_load/\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start(\"abfss://silver@scmdataset2025.dfs.core.windows.net/Logistics/\") \\\n",
    "    .awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5169661753622945,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "DatasetName",
      "width": 166
     },
     {
      "breakBefore": false,
      "name": "Quarter",
      "width": 166
     },
     {
      "breakBefore": false,
      "name": "dataset_name",
      "width": 166
     },
     {
      "breakBefore": false,
      "name": "quadrate",
      "width": 166
     }
    ]
   },
   "notebookName": "SCM (Silver)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30899c53-4ecd-49e8-b7d6-b6e72dc611cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from pyspark.sql.functions import to_date, date_format, coalesce, col,count, when, create_map, lit\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bd28f95-0817-4dfc-b508-79fc3d2827fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "creating spark section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fff343f2-961a-49c9-8513-4bfcfddde53f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DeltaTableCheckAndAppend\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5481cbdd-e966-409a-a8f5-0d6cd69a1490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "082b6754-08ad-4f11-a7e1-8e00ce9383a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, count, when\n",
    "# Define the Delta table path\n",
    "delta_path = \"abfss://silver@scmdataset2025.dfs.core.windows.net/Customer\"\n",
    "\n",
    "# Function to check if Delta table exists\n",
    "def delta_table_exists(path):\n",
    "    try:\n",
    "        files = dbutils.fs.ls(path)\n",
    "        return any(file.name == \"_delta_log/\" for file in files)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to access path: {e}\")\n",
    "        return False\n",
    "\n",
    "# Function to get the first available CSV file dynamically\n",
    "def get_csv_path(container=\"bronze\", folder=\"Static_Data\"):\n",
    "    base_path = f\"abfss://{container}@scmdataset2025.dfs.core.windows.net/{folder}/\"\n",
    "    try:\n",
    "        files = dbutils.fs.ls(base_path)\n",
    "        # Return the first CSV file found\n",
    "        for file in files:\n",
    "            if file.name.endswith(\".csv\"):\n",
    "                return base_path + file.name\n",
    "        raise FileNotFoundError(f\"No CSV file found in {base_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to list files in {base_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main logic\n",
    "if not delta_table_exists(delta_path):\n",
    "    csv_path = get_csv_path()\n",
    "    if csv_path:\n",
    "        # Read the CSV file dynamically\n",
    "        df_customers = spark.read.format(\"csv\")\\\n",
    "            .options(header=\"true\")\\\n",
    "            .options(inferSchema=\"true\")\\\n",
    "            .load(csv_path)\n",
    "\n",
    "        # Create a temporary view\n",
    "        df_customers.createOrReplaceTempView(\"temp_customers_view\")\n",
    "\n",
    "        # Check for duplicate values\n",
    "        duplicate_counts = {}\n",
    "        for column in df_customers.columns:\n",
    "            duplicate_count = df_customers.groupBy(column).count().filter(\"count > 1\").count()\n",
    "            duplicate_counts[column] = duplicate_count\n",
    "\n",
    "        for column, count in duplicate_counts.items():\n",
    "            print(f\"{column}: {count}\")\n",
    "\n",
    "        # Drop duplicate rows based on Customer_ID\n",
    "        df_customers = df_customers.dropDuplicates([\"Customer_ID\"])\n",
    "        \n",
    "\n",
    "        # Drop the \"Customer_Name\" column\n",
    "        df_customers = df_customers.drop(\"Customer_Name\")\n",
    "\n",
    "        # Write the DataFrame to the Delta table\n",
    "        df_customers.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(delta_path)\n",
    "\n",
    "        # Optimize Delta table to maintain a single file\n",
    "        spark.sql(f\"OPTIMIZE delta.`{delta_path}`\")\n",
    "\n",
    "        print(\"File saved in the silver layer\")\n",
    "        print(\"Data processed successfully.\")\n",
    "    else:\n",
    "        print(\"CSV file not found. Please check the directory.\")\n",
    "else:\n",
    "    print(\"Delta table already exists in the silver layer.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54f68cc0-69c8-41d3-baa5-945ac8fd495d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05be1fbe-fa52-4ee1-9efa-eb9787b81dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the Delta table path\n",
    "delta_path = \"abfss://silver@scmdataset2025.dfs.core.windows.net/Product\"\n",
    "\n",
    "# Check if _delta_log directory exists\n",
    "def delta_table_exists(path):\n",
    "    try:\n",
    "        files = dbutils.fs.ls(path)\n",
    "        return any(file.name == \"_delta_log/\" for file in files)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to access path: {e}\")\n",
    "        return False\n",
    "    \n",
    "# if delta_log not exist\n",
    "if not delta_table_exists(delta_path):\n",
    "    # Read the CSV file\n",
    "    df_product = spark.read.format(\"csv\")\\\n",
    "        .options(header=\"true\")\\\n",
    "        .options(inferSchema=\"true\")\\\n",
    "        .load(\"abfss://bronze@scmdataset2025.dfs.core.windows.net/Static_Data/Product_Data.csv\")\n",
    "\n",
    "    # Create a temporary view for the Product data\n",
    "    df_product.createOrReplaceTempView(\"temp_product_view\")\n",
    "\n",
    "    # Drop duplicates based on Product_ID\n",
    "    df_product = df_product.dropDuplicates([\"Product_ID\"])\n",
    "    display(df_product)\n",
    "\n",
    "    # Write data to Delta table \n",
    "    print(\"Writing data to Delta table...\")\n",
    "    df_product.write.format(\"delta\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .option(\"mergeSchema\", \"true\")\\\n",
    "        .save(delta_path)\n",
    "\n",
    "    # Optimize Delta table to maintain a single file\n",
    "    spark.sql(f\"OPTIMIZE delta.`{delta_path}`\")\n",
    "\n",
    "    print(\"Data processed successfully.\")\n",
    "else:\n",
    "    # If Delta table already exists, do nothing\n",
    "    print(\"Delta table already exists. Skipping data processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a27dcc-df76-4f85-8a49-6cc362145dfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6eabe94-1fb0-4a9b-9d29-76181881e9bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "finding last uploaded file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62bc1f1e-2a58-4353-b24f-77a830c4644e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# List the files in the directory\n",
    "files = dbutils.fs.ls(\"abfss://bronze@scmdataset2025.dfs.core.windows.net/Stream_Data/Warehouse/\")\n",
    "\n",
    "# Sort the files by modification time and take the last one\n",
    "last_file = sorted(files, key=lambda x: x.modificationTime)[-1].path\n",
    "\n",
    "display(last_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "916f0d26-f1da-403e-836e-d8c38b15a500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#read the dataset\n",
    "df_warehouse=spark.read.format(\"csv\")\\\n",
    "     .options(header=\"true\")\\\n",
    "     .options(inferSchema=\"true\")\\\n",
    "     .load(last_file)\n",
    "\n",
    "display(df_warehouse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b4aa381-5451-431e-827a-e06fae6782c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creating tempview\n",
    "df_warehouse.createOrReplaceTempView(\"temp_warehouse_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84cd5308-763f-49c6-8f8b-f8070452b7f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Adjust the date format in the 'Date' column\n",
    "df_warehouse = df_warehouse.withColumn(\n",
    "    \"Date\",\n",
    "    date_format(\n",
    "        coalesce(\n",
    "            to_date(col(\"Date\"), \"dd-MM-yyyy\"),\n",
    "            to_date(col(\"Date\"), \"dd MM yyyy\"),\n",
    "            to_date(col(\"Date\"), \"yyyy-MM-dd\"),\n",
    "            to_date(col(\"Date\"), \"yyyy-dd-MM\"),\n",
    "            to_date(col(\"Date\"), \"yyyy_dd_MM\"),\n",
    "            to_date(col(\"Date\"), \"dd_MM_yyyy\")\n",
    "        ),\n",
    "        \"dd-MM-yyyy\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "display(df_warehouse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cacbfd8-8ad4-41ff-8b26-ebcdda2d87c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "\n",
    "# Create a mapping dictionary for Warehouse ID and Location\n",
    "warehouse_mapping = {\n",
    "    \"W001\": \"New_Delhi\",\n",
    "    \"W002\": \"Lucknow\",\n",
    "    \"W003\": \"Chandigarh\",\n",
    "    \"W004\": \"Mumbai\",\n",
    "    \"W005\": \"Ahmedabad\",\n",
    "    \"W006\": \"Jaipur\",\n",
    "    \"W007\": \"Kolkata\",\n",
    "    \"W008\": \"Bhubaneswar\",\n",
    "    \"W009\": \"Patna\",\n",
    "    \"W010\": \"Bhopal\",\n",
    "    \"W011\": \"Raipur\",\n",
    "    \"W012\": \"Ranchi\",\n",
    "    \"W013\": \"Bangalore\",\n",
    "    \"W014\": \"Hyderabad\",\n",
    "    \"W015\": \"Chennai\"\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a PySpark map\n",
    "mapping_expr = create_map([lit(x) for x in chain(*warehouse_mapping.items())])\n",
    "\n",
    "# Replace values directly in the Warehouse_Location column\n",
    "df_warehouse = df_warehouse.withColumn(\n",
    "    \"Warehouse_Location\",\n",
    "    when(\n",
    "        col(\"Warehouse_ID\").isNotNull() & (df_warehouse[\"Warehouse_Location\"] != mapping_expr.getItem(col(\"Warehouse_ID\"))),\n",
    "        mapping_expr.getItem(col(\"Warehouse_ID\"))\n",
    "    ).otherwise(col(\"Warehouse_Location\"))\n",
    ")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "display(df_warehouse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cf7feb1-bb48-4f96-8cbc-7033747a6fd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "save the files in silver layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e8ae71f-89b5-44fe-947a-88b0449c8752",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the Delta table path\n",
    "delta_path = \"abfss://silver@scmdataset2025.dfs.core.windows.net/Warehouse\"\n",
    "\n",
    "# Check if _delta_log directory exists\n",
    "def delta_table_exists(path):\n",
    "    try:\n",
    "        files = dbutils.fs.ls(path)\n",
    "        return any(file.name == \"_delta_log/\" for file in files)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to access path: {e}\")\n",
    "        return False\n",
    "\n",
    "# Check and write/append accordingly\n",
    "if delta_table_exists(delta_path):\n",
    "    # Append to existing Delta table with schema merging\n",
    "    print(\"Delta table exists. Appending new data.\")\n",
    "    df_warehouse.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(delta_path)\n",
    "else:\n",
    "    # Create a new Delta table if delta table not exist\n",
    "    print(\"Delta table does not exist. Creating a new Delta table.\")\n",
    "    df_warehouse.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "# Optimize Delta table to maintain a single file\n",
    "spark.sql(f\"OPTIMIZE delta.`{delta_path}`\")\n",
    "\n",
    "print(\"Data processed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20c74112-9cd8-4f82-a875-f688b8f63efd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "# Define the Delta table path\n",
    "delta_path = \"abfss://silver@scmdataset2025.dfs.core.windows.net/Warehouse\"\n",
    "\n",
    "# Read the Delta table\n",
    "delta_df = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "# Display the file with the largest Total_Cost\n",
    "largest_file_df = delta_df\n",
    "display(largest_file_df)\n",
    "\n",
    "# Get the size of the files in the folder\n",
    "files = dbutils.fs.ls(delta_path)\n",
    "file_sizes = [(file.name, file.size) for file in files]\n",
    "display(file_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bacbf31e-fe4d-4ff1-b8cc-df3702eee8d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Supply dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "becc1876-6b97-4096-9e75-feee1175e1b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# List the files in the directory\n",
    "files = dbutils.fs.ls(\"abfss://bronze@scmdataset2025.dfs.core.windows.net/Stream_Data/Supply\")\n",
    "\n",
    "# Sort the files by modification time and take the last one\n",
    "last_file = sorted(files, key=lambda x: x.modificationTime)[-1].path\n",
    "\n",
    "display(last_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e855de26-fea7-468b-8b1f-256edd5e8525",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read the datset\n",
    "df_supply = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"inferSchema\", True)\\\n",
    "    .load(last_file)\n",
    "\n",
    "#display dataset\n",
    "\n",
    "display(df_supply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b2baf26-a468-4ac6-9f53-63e0f9f6026a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#creating tempview\n",
    "df_supply.createOrReplaceTempView(\"temp_view_supply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2591efa-4d36-4763-b452-eb03d424665b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a list to store the results\n",
    "null_counts = []\n",
    "\n",
    "# Iterate through each column and count null values\n",
    "for column in df_supply.columns:\n",
    "    null_count = df_supply.filter(col(column).isNull()).count()\n",
    "    null_counts.append((column, null_count))\n",
    "\n",
    "# Convert the list to a DataFrame for better visualization\n",
    "df_supply2= spark.createDataFrame(null_counts, [\"Column\", \"Null Count\"])\n",
    "\n",
    "# Display the null counts for each column\n",
    "display(df_supply2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eda65687-41d9-42f2-913d-7a044296fabe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#drop supplier name column\n",
    "df_1= df_supply.drop(\"Supplier_Name\")\n",
    "display(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11dd31f6-d143-4804-95ce-a75396d850f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, coalesce, date_format, to_date\n",
    "\n",
    "# Adjust the date format in the 'Date' column\n",
    "df_supply = df_1.withColumn(\n",
    "    \"Date\",\n",
    "    date_format(\n",
    "        coalesce(\n",
    "            to_date(col(\"Date\"), \"dd-MM-yyyy\"),\n",
    "            to_date(col(\"Date\"), \"dd MM yyyy\"),\n",
    "            to_date(col(\"Date\"), \"yyyy-MM-dd\"),\n",
    "            to_date(col(\"Date\"), \"yyyy-dd-MM\"),\n",
    "            to_date(col(\"Date\"), \"yyyy_dd_MM\"),\n",
    "            to_date(col(\"Date\"), \"dd_MM_yyyy\")\n",
    "        ),\n",
    "        \"dd-MM-yyyy\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "display(df_supply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "083a54ac-9dd3-431d-8d47-8113cb3205ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Define the correct mapping of Supplier_ID to Supplier_State\n",
    "correct_mapping = {\n",
    "    \"S001\": \"Delhi\",\n",
    "    \"S002\": \"Maharashtra\",\n",
    "    \"S003\": \"West_Bengal\",\n",
    "    \"S004\": \"Madhya_Pradesh\",\n",
    "    \"S005\": \"Karnataka\"\n",
    "}\n",
    "\n",
    "# Create a DataFrame with the correct mapping\n",
    "mapping_df = spark.createDataFrame(\n",
    "    [(k, v) for k, v in correct_mapping.items()],\n",
    "    [\"Supplier_ID\", \"Correct_Supplier_State\"]\n",
    ")\n",
    "\n",
    "# Join the original DataFrame with the mapping DataFrame\n",
    "df_check = df_supply.join(mapping_df, on=\"Supplier_ID\", how=\"left\")\n",
    "\n",
    "# Replace incorrect Supplier_State with the correct value\n",
    "df_check = df_check.withColumn(\n",
    "    \"Supplier_State\",\n",
    "    when(df_check[\"Supplier_State\"] != df_check[\"Correct_Supplier_State\"], df_check[\"Correct_Supplier_State\"]).otherwise(df_check[\"Supplier_State\"])\n",
    ")\n",
    "\n",
    "# Display the DataFrame with the updated Supplier_State\n",
    "display(df_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "189f5c12-d7a0-4032-a198-7f921948706a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the Delta table path\n",
    "delta_path = \"abfss://silver@scmdataset2025.dfs.core.windows.net/Supply\"\n",
    "\n",
    "# Check if _delta_log directory exists\n",
    "def delta_table_exists(path):\n",
    "    try:\n",
    "        files = dbutils.fs.ls(path)\n",
    "        return any(file.name == \"_delta_log/\" for file in files)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to access path: {e}\")\n",
    "        return False\n",
    "\n",
    "# Check and write/append accordingly\n",
    "if delta_table_exists(delta_path):\n",
    "    # Append to existing Delta table with schema merging\n",
    "    print(\"Delta table exists. Appending new data.\")\n",
    "    df_check.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(delta_path)\n",
    "else:\n",
    "    # Create a new Delta table\n",
    "    print(\"Delta table does not exist. Creating a new Delta table.\")\n",
    "    df_check.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").option(\"columnMapping\", \"name\").save(delta_path)\n",
    "\n",
    "# Optimize Delta table to maintain a single file\n",
    "spark.sql(f\"OPTIMIZE delta.`{delta_path}`\")\n",
    "\n",
    "print(\"Data processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a449ee8-66d1-4fa7-9fc1-9523d98700be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "763487d7-fd2b-4304-882d-784d72672a99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# List the files in the directory\n",
    "files = dbutils.fs.ls(\"abfss://bronze@scmdataset2025.dfs.core.windows.net/Stream_Data/Orders\")\n",
    "\n",
    "# Sort the files by modification time and take the last one\n",
    "last_file = sorted(files, key=lambda x: x.modificationTime)[-1]\n",
    "\n",
    "# Display the last file\n",
    "print(last_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "742889e4-403b-4df7-a58f-5beb6ca40405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_orders = spark.read.format(\"csv\")\\\n",
    "     .options(header=\"true\")\\\n",
    "     .options(inferSchema=\"true\")\\\n",
    "     .load(last_file.path)\n",
    "\n",
    "display(df_orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38491b0e-4493-4bfb-ad05-526b596bda68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "duplicate_counts = {}\n",
    "\n",
    "for column in df_orders.columns:\n",
    "    duplicate_values = (\n",
    "        df_orders.groupBy(column)\n",
    "        .count()\n",
    "        .filter(col(\"count\") > 1)\n",
    "        .select(col(column).alias(\"value\"), col(\"count\").alias(\"duplicate_count\"))\n",
    "    )\n",
    "    duplicate_counts[column] = duplicate_values.count()\n",
    "\n",
    "for column, count in duplicate_counts.items():\n",
    "    print(f\"{column}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c11f2e4-a13e-40cf-91cc-b95a03dfee97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_orders = df_orders.dropDuplicates()\n",
    "display(df_orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3bf89ad-5a1e-4ae0-9d73-5995b931a929",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the Delta table path\n",
    "delta_path = \"abfss://silver@scmdataset2025.dfs.core.windows.net/orders\"\n",
    "\n",
    "# Check if _delta_log directory exists\n",
    "def delta_table_exists(path):\n",
    "    try:\n",
    "        files = dbutils.fs.ls(path)\n",
    "        return any(file.name == \"_delta_log/\" for file in files)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to access path: {e}\")\n",
    "        return False\n",
    "\n",
    "# Check and write/append accordingly\n",
    "if delta_table_exists(delta_path):\n",
    "    # Append to existing Delta table with schema merging\n",
    "    print(\"Delta table exists. Appending new data.\")\n",
    "    df_orders.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").option(\"enableColumnMapping\", \"true\").save(delta_path)\n",
    "else:\n",
    "    # Create a new Delta table\n",
    "    print(\"Delta table does not exist. Creating a new Delta table.\")\n",
    "    df_orders.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "# Optimize Delta table to maintain a single file\n",
    "spark.sql(f\"OPTIMIZE delta.`{delta_path}`\")\n",
    "\n",
    "print(\"Data processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c9d3509-d5a3-444d-87de-371d7f2efff3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Logistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f66da313-f477-4f93-976b-849a7c1c71f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# List the files in the directory\n",
    "files = dbutils.fs.ls(\"abfss://bronze@scmdataset2025.dfs.core.windows.net/Stream_Data/Logistics\")\n",
    "\n",
    "# Sort the files by modification time and take the last one\n",
    "last_file = sorted(files, key=lambda x: x.modificationTime)[-1]\n",
    "\n",
    "# Display the last file\n",
    "print(last_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "957144cb-d342-4a02-b10e-a8883d39f5ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "     .options(header=\"true\")\\\n",
    "     .options(inferSchema=\"true\")\\\n",
    "     .load(last_file.path)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8540e083-da13-4c7a-ad1a-e5864c4b3492",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the Delta table path\n",
    "delta_path = \"abfss://silver@scmdataset2025.dfs.core.windows.net/Logistics\"\n",
    "\n",
    "# Check if _delta_log directory exists\n",
    "def delta_table_exists(path):\n",
    "    try:\n",
    "        files = dbutils.fs.ls(path)\n",
    "        return any(file.name == \"_delta_log/\" for file in files)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to access path: {e}\")\n",
    "        return False\n",
    "\n",
    "# Check and write/append accordingly\n",
    "if delta_table_exists(delta_path):\n",
    "    # Append to existing Delta table with schema merging\n",
    "    print(\"Delta table exists. Appending new data.\")\n",
    "    df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").option(\"enableColumnMapping\", \"true\").save(delta_path)\n",
    "else:\n",
    "    # Create a new Delta table\n",
    "    print(\"Delta table does not exist. Creating a new Delta table.\")\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "# Optimize Delta table to maintain a single file\n",
    "spark.sql(f\"OPTIMIZE delta.`{delta_path}`\")\n",
    "\n",
    "print(\"Data processed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SCM (Silver)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
